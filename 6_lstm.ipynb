{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data. Problem 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n",
      "ons anarchists advocate social relations based upon voluntary association of autonomous individuals \n",
      "identified in one eight four two and extensively excavated in one nine six three one nine six five b\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n",
    "print(text[1000:1100])\n",
    "print(text[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Unexpected character: Ã¯\n",
      "1 26 0 0\n",
      "a z y\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "print(vocabulary_size)\n",
    "first_letter = ord(string.ascii_lowercase[0])  # 97; ord() is inverse of chr()\n",
    "\n",
    "# makes 'a' - 1; 'b' - 2, etc, z - '26'\n",
    "# all other characters are 0\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "# inverse: 1 -> 'a', 2 -> 'b', 26 -> 'z'\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' relations ', 'ed to reviv', 'otographic ', 'sacred dest', 'tile daught', 'd a detaile', 'g jews mand', 'cember one ', 'nd from pre', 'e one nine ', 'facturers o', 'debody jet ', ' some of th', 'the most in', 'acts of mer', 'd from the ', ' and guerns', ' and social', 'anity vol t', 'aquinas com', 'lization an', 'on solution', 'usually mea', ' him out bu', 'ility to or', ' operation ', 'istakes of ', 'es moines t', ' his oppone', ' mailboxes ', 'ristianity ', 'pularity of', 'cument fax ', 'ht zero one', 'listing of ', 'ieutenant s', 'cs and spec', 'ison maize ', 'tal applica', 'configurati', 'rliament s ', 'istorians a', 'lc circuit ', 'whole genom', 'l language ', ' point pres', 'wo one one ', 'prise serve', 'lege newspa', 'p lewis has', 'd the econo', ' flat to ti', 'dney based ', ' negotiatio', 'the lesotho', 'ors wrote i', 'do this cla', 'matics pres', ' is represe', 'rograms mus', ' links bbc ', 'te modern d', 'especially ', 'ible the sy']\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n",
      "['ar']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297675 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "elamtvwrcyl iamib ggieeseikg emrn  iyyevnexho tapd eboxyr  si eg qjrofne guvowmh\n",
      "jj fssevsecnnintjbgztgvbxddq em ydirgysinrcvexsopmth cvsuuxnim raktasjtbiacebwy \n",
      "ayno  gotrd e nty b feptenaknw xyi xor  wagiel tthbegvspdj af twwidrs nazij oihn\n",
      "wt ca l oznnw uimgiusve ohfzpmib entrkktbntpbeisaieiyeihnegnmsonfioae t nohtev e\n",
      "lj  detto  oxxpyfetnroeujoxuiru oryrygipenzdixsetiolsvonle xea  hottwnhnycuuacfi\n",
      "================================================================================\n",
      "Validation set perplexity: 19.87\n",
      "Average loss at step 100: 2.594457 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.95\n",
      "Validation set perplexity: 10.97\n",
      "Average loss at step 200: 2.245493 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 300: 2.092966 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.002277 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 500: 1.941107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 600: 1.916144 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 700: 1.863182 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.824155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.836155 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.821852 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "================================================================================\n",
      "bcoy has cultern the cally rifferricicly quist rizgions formeqneh amoutring give\n",
      "carthic lsame cin the selomije a rompers trampostrapievetyly or b s x batle morc\n",
      "xovifix from pogume welemer heapd ins offerent liderike expideas aris ac orchata\n",
      "guten one secc an dic motions a sack marnesy decome ench of saculliaging appeste\n",
      "bacia im estulk cerm foc sopoun pistric mmevish gived dich yeang setion examilio\n",
      "================================================================================\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1100: 1.777021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.753696 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.727509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1400: 1.744472 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.734104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.742841 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1700: 1.708663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.675336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.646773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.695035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "chard rassated to the argus and batted spebrosisl of a his worked colatical cont\n",
      "je the seven of herer the controratis latter in c the wattem operen women to the\n",
      "re inqaysita crosking brod bevilles s intex domp in seein for the jougtt for thi\n",
      " dighist authudizate famally sconsed produced in one nine pealls cativious wasno\n",
      "lise bermarish hasoblejed stance see stands capt toly pha rang linghist reported\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.687310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200: 1.684900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.643800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2400: 1.661326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2500: 1.680846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.652863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2700: 1.653357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.652494 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.651323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3000: 1.651054 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "onism stanchal hiplodous changjed southetwornautherry of the depotmes the seven \n",
      "coll could in afperate eifte his the walts carned the economizg basikan doud to \n",
      "y theame fis since the medv daugs and bepiemehs us to in the loston protenttifle\n",
      "vered particularch commone improcully bowseres the firply day lived to incilled \n",
      "dubley that the syntran bamard the ana oneiono ille artacoe greackainational sty\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3100: 1.632582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3200: 1.644471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.642787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.669161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3500: 1.655524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3600: 1.667516 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3700: 1.653052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3800: 1.643083 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3900: 1.641681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.651418 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "guty minioandament nine mahapholide wakan ramol depater with ars ormition eack t\n",
      "ver the short of the amorrashor as the use fited to ch liver preabour a restanah\n",
      "y matho an and from tithul with dimital astroces the candain is his s the first \n",
      "u bylogial lamed of there fax may distrogel the constant for indailaph or is ast\n",
      "wyine one nine seven nine six one favwast spote boorca compios strather has far \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.635604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.633426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4300: 1.614363 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.609455 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.616481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.618302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.623979 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4800: 1.630892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.633516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5000: 1.607671 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "ratres is i of e kbish mikie grabit as the babain mons comionity zeros follm gat\n",
      "ed there for boenne his underchoting filioqure moath othe of hymes of end the pl\n",
      "joving the bound meaying of eight foot three ral hananh oqrimite arm gyplimian e\n",
      "rane roons of this by a messium that monsion killing rangur list to bost on beni\n",
      "age langer wide of introdulic inzrifulive and eversitar marriam in one the produ\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.603800 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5200: 1.587967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.583284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400: 1.581629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.568972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.584833 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.568151 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.583433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.571599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.546797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "ven the popular is are english fut romes tases orders of xrabs to fination must \n",
      "e mousers relations of public an artists everyent were she sird upball sexalwa b\n",
      "vershepote defined films one five four one nine seven seven zero four as belweve\n",
      "ble of the many a part by at a one eight five seven zero zero year to subathed d\n",
      "zater in the iloplany strungte is can shimple may biokoly chamsungs were have sa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.562765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6200: 1.536246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.542007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.542248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.562496 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.595670 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6700: 1.585482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6800: 1.608280 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900: 1.581242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7000: 1.582077 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "phowiable stays quich in the veryor that to mauson cograchically ramber been slo\n",
      "conds anine two mmilled and comitre formately the by numbers on server herron go\n",
      "gutils tethnographicti last is the ontred the other and wringer is guildgen late\n",
      "grams lose were not noilled a band worlly edol s placen by importania mosted von\n",
      "versing and comol word with as proportial for the idal art countures navgi teams\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # LSTM parameter definitions\n",
    "  # weight matrices to be multiplied with input variable\n",
    "  ifcox = tf.Variable(tf.truncated_normal([4, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # weight matrices multipied with output variable\n",
    "  ifcom = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # bias variables\n",
    "  b_stack = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "# Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    # stack input 4 times\n",
    "    i_stack = tf.stack([i,i,i,i])\n",
    "    o_stack = tf.stack([o,o,o,o])\n",
    "    nn_layer_in_lstm = tf.matmul(i_stack, ifcox) + tf.matmul(o_stack, ifcom) + b_stack\n",
    "    \n",
    "    input_gate = tf.sigmoid(nn_layer_in_lstm[0])\n",
    "    forget_gate = tf.sigmoid(nn_layer_in_lstm[1])\n",
    "    update = nn_layer_in_lstm[2]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(nn_layer_in_lstm[3])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294657 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "jsblawsr yqtzhjnyxfceqcj zmi vdsn  fe osgbpacpt tpofhhwzhet i  bivnzyen x s zkav\n",
      "h eoiy hvzi k w ap mip ncy asfsrqrdrranvqzdtkk  r euba  tdrwsni sbpwtdavin ydt i\n",
      "iejdxfgh voltu nticdv bznizfgs sadjiiluoxrx lssncggphmfvlitewonsfw qfyzsrxg gota\n",
      "eqx  p  bxdki dlovhpsecvglfsrvbchbsh ww obekxq hll t nhexzandlph ubsysyhq  mm  t\n",
      "zkycdsmoohiejpnoprvje ubyoldnxucjszqhtseciaio bduyjhokbb eigte a ixl r vdeihig o\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.589058 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.09\n",
      "Validation set perplexity: 11.91\n",
      "Average loss at step 200: 2.245007 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300: 2.078595 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.99\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 400: 2.028362 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.969470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 600: 1.884683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.862440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 800: 1.852300 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.830859 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.828088 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "================================================================================\n",
      "d han apations the centivelifity monary comple loumpuring one eight three has wi\n",
      "jamic pequations invouned in the treat wht bathor creter pook his plicuidia nevi\n",
      "haid  one one peried of the one nine one seven terh greate the tharie of tiothar\n",
      "an manoned b ogid zero six lowist usmovie promere the bloarge engly and and some\n",
      "rautsionte oriaut dew helary clud aniated to a gnomd beind alender one any d pou\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.789088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1200: 1.759239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1300: 1.749097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.749538 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1500: 1.733451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1600: 1.718981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1700: 1.702607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1800: 1.675716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.684145 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.669402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "f fotneconona the specura damench now edent but j hisal cqusite culistan ton ho \n",
      "pancity thig and an mine modusional guime spite s demichar undement by nan can s\n",
      "els soxaits of a precrunimy his where comked command lowalf centrosity sunew tra\n",
      "k seperulared for deren then underces any whrch the spe were backet genets phory\n",
      "ing kosh when i zenus different bos woil retlis of it wise legrekever mainh arei\n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2100: 1.675336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.691808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2300: 1.692752 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2400: 1.668823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2500: 1.680190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2600: 1.660495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2700: 1.674959 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2800: 1.675897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2900: 1.666010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 3000: 1.672887 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "z dines freementity english uska is batans coamman bolgries land guplops one eit\n",
      "pured of the chress intides as the one nine nine eight chriber and mayien captre\n",
      "ult overous and entire injest be centure bed juthermier hisd cella so nive of to\n",
      "hana delegam a decosslonoy sea rolius the pissionters do north he mossel weolth \n",
      "n indes one nine four seven mandzees computets of five two zero zero two zero tw\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3100: 1.649393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3200: 1.622859 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.638364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3400: 1.624256 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.667664 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3600: 1.643785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3700: 1.649621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3800: 1.650182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3900: 1.639830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4000: 1.633438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "hands the moush websit ruquevely war bn such the subred against about sceme loza\n",
      "ifie has steg had and hbs s free kerservating the minife descioup wheraged some \n",
      "xicte one five table commained his paterial is met by suppirerege goldst the nub\n",
      "poring aing confite between gififedents the effect for the zero t more om when o\n",
      "lic some in yous zeria impered termbers and one eight nine five exvers and a man\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4100: 1.610228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4200: 1.608532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4300: 1.614744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4400: 1.605799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4500: 1.630511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4600: 1.614343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4700: 1.611461 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4800: 1.600713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.610382 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5000: 1.603735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      " or is a sietinitiusl o being his heigh printwallete persle features out sugrte \n",
      "mendi rigg or the first two zero zero s of two greya and af to thatbenting spatr\n",
      "jougrimen on on own we confrication of a from musifices the dool of kzook shils \n",
      "t oppared sphretwork for in the within won to the comic republicic an it fold pl\n",
      "b one zero zeron s depression by the yusterlar duar agast chanre unverses contyr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5100: 1.584007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.587124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.580278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.579389 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5500: 1.576843 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.552206 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.569133 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5800: 1.590642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.565991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.576792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "nete cild and early jacmentroyac war my is a topiled are de cowernessyy same ref\n",
      "vels old the name with of infienied raditor corre of many aphovige cha there jap\n",
      "on which poels army are the mainctical oebyoe cannov it witeoner winstime italen\n",
      "pre alocuotions on viilitoria be and may commences dagin feature at ltv in and a\n",
      "jown agreary generrict inventey is is one dioupdal epopps staryer as supere of t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.567064 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.579862 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6300: 1.576243 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.556514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6500: 1.540377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.589979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.554908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6800: 1.564070 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6900: 1.556587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.579418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ns the under the with strippet deation number of both maptic extent to of a usig\n",
      "can b how and they classing air houna while cluct atom ghing viex poly at the pa\n",
      "unt of deschular mater compoming the daus propeate divolw by we skaking casions \n",
      "d to addodbe varthes armisted and craine is seffect curcrable bounist and triver\n",
      "d the sp remoing vible in m of permighd in the magic an common the paten all pop\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing stacking and unstacking TF tensors \n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant([1,2,3,4,5,6], shape=[2,3])\n",
    "b = tf.constant([7,8,9,10,11,12], shape=[3,2])\n",
    "\n",
    "print(\"a*b\\n: {}\".format(tf.matmul(a,b).eval()))\n",
    "\n",
    "c = tf.constant([13, 14, 15, 16, 17, 18], shape=[3,2])\n",
    "print(\"\\na*c\\n: {}\".format(tf.matmul(a, c).eval()))\n",
    "\n",
    "d = tf.constant([19,20,21,22,23,24], shape=[3,2])\n",
    "print(\"\\na*d\\n: {}\".format(tf.matmul(a, d).eval()))\n",
    "\n",
    "a_stack = tf.stack([a,a,a])\n",
    "print(\"\\na stack\\n: {}\".format(a_stack.eval()))\n",
    "\n",
    "bcd_stack = tf.stack([b,c,d])\n",
    "print(\"\\nbcd stack\\n: {}\".format(bcd_stack.eval()))\n",
    "\n",
    "# matrix multiply result - list of tensors\n",
    "print(\"mm: {}\".format(tf.matmul(a_stack, bcd_stack).eval()))\n",
    "\n",
    "# unstack (don't actually need this step). The result is the same as mm above\n",
    "un = tf.unstack(tf.matmul(a_stack, bcd_stack).eval())\n",
    "print(\"\\nunstacked tensors:\")\n",
    "for i in un:\n",
    "    print(\"\\n {}\".format(i.eval()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' relations ', 'ed to reviv', 'otographic ', 'sacred dest', 'tile daught', 'd a detaile', 'g jews mand', 'cember one ', 'nd from pre', 'e one nine ', 'facturers o', 'debody jet ', ' some of th', 'the most in', 'acts of mer', 'd from the ', ' and guerns', ' and social', 'anity vol t', 'aquinas com', 'lization an', 'on solution', 'usually mea', ' him out bu', 'ility to or', ' operation ', 'istakes of ', 'es moines t', ' his oppone', ' mailboxes ', 'ristianity ', 'pularity of', 'cument fax ', 'ht zero one', 'listing of ', 'ieutenant s', 'cs and spec', 'ison maize ', 'tal applica', 'configurati', 'rliament s ', 'istorians a', 'lc circuit ', 'whole genom', 'l language ', ' point pres', 'wo one one ', 'prise serve', 'lege newspa', 'p lewis has', 'd the econo', ' flat to ti', 'dney based ', ' negotiatio', 'the lesotho', 'ors wrote i', 'do this cla', 'matics pres', ' is represe', 'rograms mus', ' links bbc ', 'te modern d', 'especially ', 'ible the sy']\n",
      "[' an']\n",
      "['nar']\n",
      "['rch']\n",
      "['his']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "  # LSTM parameter definitions\n",
    "  # stack of weight matrices ix, fx, cx, ox to be multiplied with input variable\n",
    "  ifcox = tf.Variable(tf.truncated_normal([4, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  # stack of weight matrices im, fm, cm, om to be multipied with output variable\n",
    "  ifcom = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # stack of bias variables\n",
    "  b_stack = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "# Definition of the cell computation\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"   \n",
    "    # stack input 4 times\n",
    "    i_stack = tf.stack([i,i,i,i])\n",
    "    o_stack = tf.stack([o,o,o,o])\n",
    "    nn_layer_in_lstm = tf.matmul(i_stack, ifcox) + tf.matmul(o_stack, ifcom) + b_stack  \n",
    "    input_gate = tf.sigmoid(nn_layer_in_lstm[0])\n",
    "    forget_gate = tf.sigmoid(nn_layer_in_lstm[1])\n",
    "    update = nn_layer_in_lstm[2]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(nn_layer_in_lstm[3])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by two characters\n",
    "\n",
    "  # Unrolled LSTM loop\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(embeddings, bigram_index)\n",
    "    \n",
    "    # dropout on the input/output (not on past/future gates).\n",
    "    drop_i = tf.nn.dropout(i_embed, 0.7)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    drop_o = tf.nn.dropout(output, 0.7)\n",
    "    outputs.append(drop_o)\n",
    "\n",
    "  # State saving across unrollings\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.  \n",
    "  sample_input = list()\n",
    "  for _ in range(2):  \n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_embeddings = tf.nn.embedding_lookup(embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embeddings, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292109 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.90\n",
      "================================================================================\n",
      "ngvnx  qe lalc j  e naoaip fh  danrjw qsib qti n  baprbhvwwvpgv densdncgjopkprbf \n",
      "wm zersu tlg zc otuvzznfna  zdu re mlir gkvxga sap  naybmvort h mxhz vw  tprzysni\n",
      "vozcv aeo yqudkeclrjza sldig oy w ozf  s  r aooqe  lcdnrgtd aag sek  u  lylip al \n",
      "xhu grszm j sqkdvo fabtgeoqxcnc moy  pam u ksa mg i yaoctuh tvimtjsd ehanrre ofg \n",
      "dvo u ynmsniiphv cswm x lhp zi ddngp   n pg arq j i  xhiexd dzyj zaatfaj ix ivrnd\n",
      "================================================================================\n",
      "Validation set perplexity: 21.35\n",
      "Average loss at step 100: 2.454185 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 200: 2.183831 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.11\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 300: 2.114164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.066419 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 500: 2.031330 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 600: 2.043476 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 700: 1.999502 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 800: 1.980432 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 900: 1.996745 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1000: 2.004671 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "================================================================================\n",
      "yundily fecommunfnrok lywa mecs ind thise defdt anted terwas zero zero faxmmple s\n",
      "a mautoripmansk nok difficiser frevole one zero one nine four two ecound mehrum t\n",
      "jr wortrous the they lediffentned light reggelg infort redimich dog proversad a i\n",
      "ji to four ares neweats lear in four two enfbippessed suisw pedere persnety noper\n",
      "of nehrunic encome histing alprosle to the deverhe in est sourmarn morua lecamest\n",
      "================================================================================\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 1100: 1.963661 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1200: 1.955147 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1300: 1.929469 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 1400: 1.961153 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1500: 1.956294 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 1600: 1.969744 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1700: 1.938058 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1800: 1.906994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 1900: 1.881487 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2000: 1.933137 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "================================================================================\n",
      "bnly ano knine gover in consophegation conter one nine nix nine zero was speus de\n",
      "mong his the concess ponstras commens prociwording houth engerbe miccefter as mus\n",
      "ey ssed a has four ave mest gosce curimples wounsty for show for in of the he ved\n",
      "wonity two four zero ed meing three ver six thring the a liness in see gerysts sa\n",
      "sgner the in by of bels jost commody the meazhcy meew intexpling and americis one\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 2100: 1.941461 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2200: 1.931736 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2300: 1.897471 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2400: 1.909329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2500: 1.926150 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 2600: 1.910483 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 2700: 1.913805 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2800: 1.909470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 2900: 1.906180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3000: 1.895945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "================================================================================\n",
      "hrom other have hoi was of form satherd meorms offemering he reging imptter one s\n",
      "wb one caot esign way the would dauta formegaels of the makend edime well would w\n",
      "grot is a prosupers one nine five seven zero three the combaston two five other f\n",
      "yan and the varcition his words march poperenis base a rebernments was and exampl\n",
      "vrhaofsures sea coum for dezate shower wer as soup one laine the the novaty of ba\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 3100: 1.881199 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 3200: 1.898971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 3300: 1.893814 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 3400: 1.926481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 3500: 1.912355 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3600: 1.920530 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 3700: 1.908984 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 3800: 1.892162 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 3900: 1.901571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4000: 1.903731 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "================================================================================\n",
      "ys of oons casion fii faftriarzos urchusan in truks ying in unatter of heregolard\n",
      "ek extlating to backudistret of cuidnn slet the one three one sountrakhar in that\n",
      "but but esouch only of consity pild in days bleak bablute steeting word of may es\n",
      "unding the foregood four the which serd lisage teans unificar for at are ction of\n",
      "lude of provelific addrevationary stelphol a memproken ros will stitutore eviment\n",
      "================================================================================\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 4100: 1.885969 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 4200: 1.900821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 4300: 1.877337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 4400: 1.872050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 4500: 1.876027 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 4600: 1.873828 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 4700: 1.894254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 4800: 1.890966 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 4900: 1.891137 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5000: 1.855944 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.26\n",
      "================================================================================\n",
      "sion exich muth sapt is seeted horman and cupenhant noteluideon ance one came ear\n",
      "ld fuland dical r of musics in one is d goes roas palnute ean rations wary the qu\n",
      "vole hoid one nine five six the fell assitnes ganunter has insultan aqeachrefed p\n",
      "fjfor the romunal to authel x has a letbaavinations woir meditian primests kable \n",
      "ld the in the instrondince j naleenry endeo repamerions and yath one and caneed s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5100: 1.865817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 5200: 1.865984 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 5300: 1.842913 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 5400: 1.844790 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 5500: 1.843777 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 5600: 1.842654 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 5700: 1.842116 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 5800: 1.841703 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 5900: 1.843934 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6000: 1.813729 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.49\n",
      "================================================================================\n",
      "jor reprec amr shonshirnish the coutch impallen blogy bapietbant of the after gam\n",
      "hle one six eight jew the right the nation of gamerce annis parts lawdge conterds\n",
      "ms s cothiciiite by shorm welranized ireda was a reloping a and vimoth interce wo\n",
      "tqs boqcy of one nine four four kimentevii of pries nounds and ments kide is name\n",
      "hnaping moder to compers of love hunan e ceted that intendary a cbgraduch is scho\n",
      "================================================================================\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6100: 1.823939 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6200: 1.803143 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 6300: 1.806178 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6400: 1.800953 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 6500: 1.823548 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6600: 1.852061 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6700: 1.839388 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 6800: 1.863128 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 6900: 1.837989 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 7000: 1.837765 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.65\n",
      "================================================================================\n",
      "ch recontistiantmepelst relation as lacked winto kged their an in the knine the n\n",
      "tpdebrist und we vyls mad or gorginvelation ssidly um is havational persettle gre\n",
      "vt iide are lase kedmars a s intred dears areight arm on hilly canal and rain thi\n",
      "fhratives steling lawaugted a m of in dicerzael was alivieval fitment these the h\n",
      "ffamation in ructed ine ired battle righh it is to cads this fitual dell a defren\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "#   for step in range(1):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])  \n",
    "        \n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "            \n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],  sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
